LLM çš„çœŸå®å·¥ä½œæ–¹å¼ï¼š

ä½ åªéœ€è¦è®°ä½ä¸€å¥è¯ï¼š

**LLM = åœ¨å½“å‰ä¸Šä¸‹æ–‡é‡Œï¼Œé¢„æµ‹â€œä¸‹ä¸€ä¸ª token æœ€å¯èƒ½æ˜¯ä»€ä¹ˆâ€**

è¿™æ„å‘³ç€ï¼š

+ âŒ å®ƒä¸æ˜¯åœ¨â€œæ€è€ƒçœŸç†â€

+ âŒ å®ƒä¸çŸ¥é“å¯¹é”™

+ âœ… å®ƒåªæ˜¯åœ¨å»¶ç»­ä¸€ç§çœ‹èµ·æ¥â€œåˆç†â€çš„æ¨¡å¼

è¿™ç›´æ¥å¯¼è‡´ 3 ä¸ª Agent æ ¸å¿ƒé—®é¢˜

| ç°è±¡  | åŸå›             |
| --- | ------------- |
| å¹»è§‰  | æ²¡æœ‰â€œä¸çŸ¥é“â€çš„æ¦‚å¿µ    |
| è·‘å  | Context è¢«æ±¡æŸ“   |
| ä¸ç¨³å®š | Sampling æ˜¯æ¦‚ç‡çš„ |

ğŸ‘‰ Agent å·¥ç¨‹çš„æœ¬è´¨ï¼šç»™æ¨¡å‹åŠ â€œæŠ¤æ â€

## å®æˆ˜

ğŸ¯ ç›®æ ‡

è®©ä½ â€œæ„Ÿå—åˆ°â€æ¨¡å‹çš„éšæœºæ€§ã€‚

**ä½ è¦åšçš„äº‹**

+ åŒä¸€ä¸ª Prompt

+ ä¸åŒ temperature

+ è¿ç»­è·‘ 10 æ¬¡

è§‚å¯Ÿï¼š

+ é€»è¾‘æ˜¯å¦ä¸€è‡´

+ ç»“è®ºæ˜¯å¦ç¨³å®š

**ä½ åº”è¯¥å¾—å‡ºçš„ç»“è®º**

Agent ä¸èƒ½é ä¸€æ¬¡å›ç­”

å¿…é¡»é ç»“æ„ã€å¾ªç¯å’Œåé¦ˆ

ä»£ç ï¼š

```python
from openai import OpenAI

client = OpenAI()

PROMPT = "è¯·ç”¨ä¸‰ç‚¹è¯´æ˜ï¼šä¸ºä»€ä¹ˆé•¿æœŸåšæŒè¿åŠ¨æœ‰åŠ©äºæé«˜ä¸€ä¸ªäººçš„è‡ªæ§åŠ›ï¼Ÿ"

def run_experiment(temperature, runs=10):
    print(f"\n====== Temperature = {temperature} ======\n")
    for i in range(runs):
        response = client.chat.completions.create(
            model="gpt-4.1-mini",
            temperature=temperature,
            messages=[
                {"role": "user", "content": PROMPT}
            ]
        )
        print(f"--- Run {i+1} ---")
        print(response.choices[0].message.content)
        print()

if __name__ == "__main__":
    for temp in [0.1, 0.7, 1.2]:
        run_experiment(temp)
```
